{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyjtsHVQXl91",
        "outputId": "f369b00d-f0d3-48b4-81af-195e22d87d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-10 14:20:05--  https://raw.githubusercontent.com/QwanXiong/NN_properties/main/enrg_s0_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76000 (74K) [text/plain]\n",
            "Saving to: ‚Äòenrg_s0_train.txt.3‚Äô\n",
            "\n",
            "enrg_s0_train.txt.3 100%[===================>]  74.22K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-04-10 14:20:05 (85.1 MB/s) - ‚Äòenrg_s0_train.txt.3‚Äô saved [76000/76000]\n",
            "\n",
            "--2023-04-10 14:20:05--  https://raw.githubusercontent.com/QwanXiong/NN_properties/main/grad_1_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 766545 (749K) [text/plain]\n",
            "Saving to: ‚Äògrad_1_train.txt.3‚Äô\n",
            "\n",
            "grad_1_train.txt.3  100%[===================>] 748.58K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-04-10 14:20:06 (61.9 MB/s) - ‚Äògrad_1_train.txt.3‚Äô saved [766545/766545]\n",
            "\n",
            "--2023-04-10 14:20:06--  https://raw.githubusercontent.com/QwanXiong/NN_properties/main/geom.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1404465 (1.3M) [text/plain]\n",
            "Saving to: ‚Äògeom.txt.3‚Äô\n",
            "\n",
            "geom.txt.3          100%[===================>]   1.34M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-04-10 14:20:06 (175 MB/s) - ‚Äògeom.txt.3‚Äô saved [1404465/1404465]\n",
            "\n",
            "--2023-04-10 14:20:06--  https://raw.githubusercontent.com/QwanXiong/NN_properties/main/nacdr_1_2_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 812339 (793K) [text/plain]\n",
            "Saving to: ‚Äònacdr_1_2_train.txt.1‚Äô\n",
            "\n",
            "nacdr_1_2_train.txt 100%[===================>] 793.30K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-04-10 14:20:07 (113 MB/s) - ‚Äònacdr_1_2_train.txt.1‚Äô saved [812339/812339]\n",
            "\n",
            "--2023-04-10 14:20:07--  https://raw.githubusercontent.com/QwanXiong/NN_properties/main/nac_1_2_purified_from_NAC_fit_dE.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1401517 (1.3M) [text/plain]\n",
            "Saving to: ‚Äònac_1_2_purified_from_NAC_fit_dE.txt‚Äô\n",
            "\n",
            "nac_1_2_purified_fr 100%[===================>]   1.34M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-04-10 14:20:07 (181 MB/s) - ‚Äònac_1_2_purified_from_NAC_fit_dE.txt‚Äô saved [1401517/1401517]\n",
            "\n",
            "--2023-04-10 14:20:07--  https://raw.githubusercontent.com/QwanXiong/NN_properties/main/nacdr_1_2_purified_from_NAC_fit.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1473517 (1.4M) [text/plain]\n",
            "Saving to: ‚Äònacdr_1_2_purified_from_NAC_fit.txt.2‚Äô\n",
            "\n",
            "nacdr_1_2_purified_ 100%[===================>]   1.40M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-04-10 14:20:07 (229 MB/s) - ‚Äònacdr_1_2_purified_from_NAC_fit.txt.2‚Äô saved [1473517/1473517]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/QwanXiong/NN_properties/main/enrg_s0_train.txt\n",
        "!wget https://raw.githubusercontent.com/QwanXiong/NN_properties/main/grad_1_train.txt\n",
        "!wget https://raw.githubusercontent.com/QwanXiong/NN_properties/main/geom.txt\n",
        "!wget https://raw.githubusercontent.com/QwanXiong/NN_properties/main/nacdr_1_2_train.txt\n",
        "!wget https://raw.githubusercontent.com/QwanXiong/NN_properties/main/nac_1_2_purified_from_NAC_fit_dE.txt\n",
        "!wget https://raw.githubusercontent.com/QwanXiong/NN_properties/main/nacdr_1_2_purified_from_NAC_fit.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "bohr_to_ang = 0.529177"
      ],
      "metadata": {
        "id": "0yFmEf6jYNp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geom = np.loadtxt('geom.txt')\n",
        "geom = geom.reshape((4000,6,3),order='C')*bohr_to_ang\n",
        "enrg_s0 = np.loadtxt('enrg_s0_train.txt').reshape(4000,1)\n",
        "grad_s0 = np.loadtxt('grad_1_train.txt').reshape((4000,6,3),order='C')/bohr_to_ang\n",
        "z = np.array([6,7,1,1,1,1],dtype='uint8')\n",
        "nac_s0_s1 = np.loadtxt('nac_1_2_purified_from_NAC_fit_dE.txt').reshape((4000,6,3),order='C')/bohr_to_ang\n",
        "#nac_s0_s1 = np.loadtxt('nacdr_1_2_train.txt').reshape((4000,6,3),order='C')/bohr_to_ang\n"
      ],
      "metadata": {
        "id": "qYJbRfSyYaIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#de/dang = de/dbohr*dbohr/dang = 1/bohr_to_ang*de/dang\n",
        "#bohr = ang/bohr_to_ang"
      ],
      "metadata": {
        "id": "g1ATHfAPBgr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forces_s0 = -np.copy(grad_s0)\n",
        "forces_nac = -np.copy(nac_s0_s1)"
      ],
      "metadata": {
        "id": "HfjDXnJMqDhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forces_s0+grad_s0"
      ],
      "metadata": {
        "id": "by7YydeeqK3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geom_train,geom_test, enrg_s0_train, enrg_s0_test,nac_s0_s1_train,nac_s0_s1_test = \\\n",
        "train_test_split(geom,enrg_s0,forces_nac,test_size=0.1,random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "xkKRVpa2eXh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nac_s0_s1_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7aVKXyJe0e_",
        "outputId": "4ce1dd4e-cab5-4cc2-9176-72688bbb7505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3600, 6, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oe1DNn-lcgd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ch2nh2+_pot_train.npz','wb') as fil:\n",
        "  np.savez(fil,E=enrg_s0_train,F=nac_s0_s1_train,z=z,R=geom_train )\n",
        "\n",
        "with open('ch2nh2+_pot_test.npz','wb') as fil:\n",
        "  np.savez(fil,E=enrg_s0_test,F=nac_s0_s1_test,z=z,R=geom_test )"
      ],
      "metadata": {
        "id": "sjCln0j3ZbKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with np.load('ch2nh2+_pot_test.npz') as fil:\n",
        "  print(fil.files)\n",
        "  print(fil['R'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNEFsHG7bywq",
        "outputId": "86af4f14-61a0-4859-c795-5b0021bf939b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['E', 'F', 'z', 'R']\n",
            "(400, 6, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# install wandb\n",
        "!pip install wandb\n",
        "# install nequip\n",
        "!git clone --depth 1 \"https://github.com/mir-group/nequip.git\"\n",
        "!pip install nequip/\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "import numpy as np\n",
        "import torch\n",
        "from ase.io import read, write\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "9AWUhU0xh0CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp nequip/configs/example.yaml nequip/configs/example_ch2nh2.yaml"
      ],
      "metadata": {
        "id": "m3wHdzd9iR6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./results\n",
        "!nequip-train nequip/configs/example_ch2nh2.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzyz-bJQj4yA",
        "outputId": "871f5f2f-8bfc-43d3-cd69-2fa616447fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-mouse-538553\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230410_142108-qvdpzm2m\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexample-run-toluene\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-538553/toluene-example?apiKey=2321139e10ca7f1703eb25e7939779e24e5b82b2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-538553/toluene-example/runs/qvdpzm2m?apiKey=2321139e10ca7f1703eb25e7939779e24e5b82b2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "Loaded data: Batch(batch=[21600], cell=[3600, 3, 3], edge_cell_shift=[107904, 3], edge_index=[2, 107904], forces=[21600, 3], pbc=[3600, 3], pos=[21600, 3], ptr=[3601], total_energy=[3600, 1])\n",
            "    processed data size: ~3.72 MB\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type NpzDataset(3600)...\n",
            "Replace string dataset_forces_rms to 0.04437750205397606\n",
            "Replace string dataset_per_atom_total_energy_mean to 0.023284252732992172\n",
            "Atomic outputs are scaled by: [H, C, N: 0.044378], shifted by [H, C, N: 0.023284].\n",
            "Replace string dataset_forces_rms to 0.04437750205397606\n",
            "Initially outputs are globally scaled by: 0.04437750205397606, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "Number of weights: 392824\n",
            "Number of trainable weights: 392824\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      0    10        0.836        0.836       0.0258       0.0406       0.0218       0.0381       0.0295       0.0298       0.0369       0.0513       0.0426       0.0436       0.0905       0.0151\n",
            "      0    20        0.811        0.811       0.0269         0.04       0.0229       0.0383       0.0316        0.031       0.0363       0.0505       0.0419       0.0429       0.0756       0.0126\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    6.788    0.005        0.936        0.936       0.0271       0.0429       0.0233       0.0361       0.0332       0.0308       0.0392       0.0506       0.0484       0.0461       0.0802       0.0134\n",
            "Wall time: 6.790616731999762\n",
            "! Best model        0    0.936\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      1    10         1.02         1.02       0.0282       0.0447       0.0281       0.0247       0.0319       0.0282       0.0464       0.0371       0.0449       0.0428       0.0894       0.0149\n",
            "      1    20        0.991        0.991       0.0279       0.0442       0.0277       0.0305        0.026       0.0281        0.046       0.0441       0.0361       0.0421       0.0922       0.0154\n",
            "      1    30        0.749        0.749       0.0245       0.0384       0.0227       0.0279       0.0283       0.0263       0.0378       0.0383       0.0408        0.039        0.102       0.0169\n",
            "      1    40        0.939        0.939       0.0254        0.043       0.0274       0.0218        0.021       0.0234       0.0477       0.0332       0.0298       0.0369        0.102       0.0169\n",
            "      1    50        0.879        0.879       0.0253       0.0416       0.0272       0.0222       0.0209       0.0234       0.0461       0.0319       0.0294       0.0358       0.0868       0.0145\n",
            "      1    60        0.949        0.949       0.0259       0.0432       0.0263       0.0281        0.022       0.0255       0.0455       0.0439       0.0318       0.0404          0.1       0.0167\n",
            "      1    70        0.787        0.787       0.0254       0.0394       0.0248       0.0257       0.0275        0.026       0.0407        0.036       0.0373        0.038       0.0974       0.0162\n",
            "      1    80        0.661        0.661       0.0218       0.0361       0.0215       0.0225       0.0221        0.022       0.0385       0.0304       0.0309       0.0333        0.108       0.0181\n",
            "      1    90        0.982        0.982       0.0267        0.044       0.0282       0.0262       0.0211       0.0252       0.0476       0.0395       0.0313       0.0395        0.115       0.0192\n",
            "      1   100         0.77         0.77       0.0238       0.0389       0.0243       0.0232       0.0222       0.0232       0.0423       0.0316       0.0308       0.0349       0.0878       0.0146\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      1    10        0.572        0.572         0.02       0.0336       0.0198       0.0196       0.0212       0.0202       0.0358       0.0279       0.0293        0.031        0.106       0.0177\n",
            "      1    20        0.638        0.638       0.0233       0.0354       0.0221       0.0253       0.0259       0.0244        0.036       0.0339       0.0349       0.0349       0.0906       0.0151\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               1   29.364    0.005         0.85         0.85       0.0253       0.0409       0.0248        0.027       0.0256       0.0258       0.0424       0.0389       0.0365       0.0393       0.0936       0.0156\n",
            "! Validation          1   29.364    0.005        0.676        0.676       0.0223       0.0365       0.0219       0.0235       0.0225       0.0226       0.0386       0.0324       0.0311        0.034       0.0982       0.0164\n",
            "Wall time: 29.365882303000035\n",
            "! Best model        1    0.676\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      2    10         0.82         0.82       0.0219       0.0402       0.0239       0.0199       0.0161         0.02       0.0455       0.0297       0.0231       0.0328        0.095       0.0158\n",
            "      2    20        0.735        0.735        0.022       0.0381       0.0236        0.016       0.0214       0.0203       0.0424       0.0236       0.0307       0.0322        0.095       0.0158\n",
            "      2    30        0.611        0.611       0.0221       0.0347       0.0201       0.0287       0.0237       0.0241       0.0341       0.0379       0.0334       0.0352        0.126        0.021\n",
            "      2    40         0.83         0.83       0.0247       0.0404       0.0239        0.027       0.0258       0.0256       0.0424       0.0376       0.0346       0.0382        0.108       0.0179\n",
            "      2    50        0.781        0.781       0.0231       0.0392        0.023       0.0234       0.0229       0.0231       0.0419       0.0325       0.0337        0.036        0.123       0.0205\n",
            "      2    60         0.63         0.63       0.0221       0.0352       0.0204       0.0249        0.026       0.0238       0.0351       0.0341       0.0367       0.0353        0.117       0.0195\n",
            "      2    70        0.918        0.918       0.0249       0.0425       0.0254       0.0251       0.0224       0.0243       0.0459       0.0342       0.0355       0.0385        0.084        0.014\n",
            "      2    80        0.869        0.869       0.0248       0.0414       0.0273       0.0201       0.0199       0.0224       0.0463       0.0285       0.0296       0.0348       0.0878       0.0146\n",
            "      2    90        0.884        0.884       0.0247       0.0417       0.0283       0.0208       0.0141       0.0211       0.0476       0.0304       0.0211        0.033       0.0892       0.0149\n",
            "      2   100        0.828        0.828        0.024       0.0404       0.0254       0.0197       0.0223       0.0225       0.0442       0.0281       0.0342       0.0355       0.0868       0.0145\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      2    10        0.528        0.528       0.0188       0.0322       0.0193       0.0181       0.0176       0.0183       0.0353       0.0247       0.0251       0.0284        0.107       0.0178\n",
            "      2    20        0.581        0.581       0.0216       0.0338       0.0218       0.0179       0.0246       0.0215        0.036       0.0265       0.0316       0.0313        0.088       0.0147\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               2   47.716    0.005        0.754        0.754        0.023       0.0385       0.0239       0.0216       0.0208       0.0221        0.042       0.0309         0.03       0.0343       0.0962        0.016\n",
            "! Validation          2   47.716    0.005        0.639        0.639       0.0211       0.0355       0.0214       0.0208       0.0201       0.0208       0.0383       0.0291       0.0286        0.032       0.0979       0.0163\n",
            "Wall time: 47.717521058999864\n",
            "! Best model        2    0.639\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      3    10        0.689        0.689        0.023       0.0368       0.0235       0.0209       0.0235       0.0226         0.04       0.0275       0.0311       0.0329        0.108        0.018\n",
            "      3    20        0.895        0.895       0.0243        0.042       0.0267         0.02       0.0188       0.0218       0.0465       0.0294       0.0325       0.0361       0.0787       0.0131\n",
            "      3    30        0.614        0.614       0.0205       0.0348       0.0197       0.0215       0.0229       0.0214        0.036        0.029       0.0352       0.0334       0.0875       0.0146\n",
            "      3    40        0.812        0.812       0.0225         0.04       0.0251       0.0181       0.0165       0.0199       0.0455       0.0268       0.0245       0.0322        0.091       0.0152\n",
            "      3    50        0.446        0.446       0.0186       0.0296       0.0179       0.0219       0.0183       0.0194       0.0303       0.0286       0.0278       0.0289       0.0936       0.0156\n",
            "      3    60        0.707        0.707       0.0229       0.0373       0.0223       0.0231       0.0253       0.0235       0.0379       0.0341       0.0381       0.0367       0.0886       0.0148\n",
            "      3    70        0.924        0.924        0.025       0.0426       0.0276       0.0195       0.0196       0.0223       0.0483       0.0282       0.0283       0.0349       0.0771       0.0128\n",
            "      3    80          0.7          0.7       0.0199       0.0371       0.0203       0.0183       0.0196       0.0194       0.0405       0.0271        0.031       0.0329       0.0816       0.0136\n",
            "      3    90        0.882        0.882       0.0237       0.0417        0.027       0.0172       0.0172       0.0205       0.0477       0.0257       0.0253       0.0329       0.0882       0.0147\n",
            "      3   100        0.732        0.732       0.0229        0.038       0.0239       0.0188       0.0229       0.0219       0.0408       0.0274       0.0352       0.0345       0.0992       0.0165\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      3    10        0.514        0.514       0.0185       0.0318       0.0191        0.017       0.0175       0.0179       0.0352       0.0232       0.0242       0.0275        0.105       0.0175\n",
            "      3    20        0.556        0.556       0.0207       0.0331       0.0212       0.0159       0.0234       0.0202       0.0357       0.0238       0.0299       0.0298       0.0861       0.0144\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               3   65.930    0.005        0.736        0.736       0.0225       0.0381       0.0237       0.0202       0.0199       0.0213       0.0418       0.0295       0.0288       0.0334       0.0947       0.0158\n",
            "! Validation          3   65.930    0.005        0.627        0.627       0.0207       0.0351       0.0211       0.0198       0.0196       0.0202       0.0382       0.0278       0.0283       0.0314        0.095       0.0158\n",
            "Wall time: 65.93149451399995\n",
            "! Best model        3    0.627\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      4    10        0.984        0.984       0.0249        0.044       0.0281       0.0182       0.0192       0.0218       0.0492       0.0317       0.0309       0.0372       0.0863       0.0144\n",
            "      4    20        0.639        0.639       0.0215       0.0355       0.0224       0.0213       0.0182       0.0206       0.0385       0.0289        0.028       0.0318        0.103       0.0172\n",
            "      4    30        0.838        0.838       0.0249       0.0406       0.0262       0.0233       0.0214       0.0236       0.0442       0.0316        0.033       0.0363        0.095       0.0158\n",
            "      4    40        0.836        0.836       0.0238       0.0406       0.0259       0.0196       0.0192       0.0216       0.0458       0.0274       0.0274       0.0335       0.0849       0.0141\n",
            "      4    50        0.795        0.795       0.0223       0.0396       0.0253       0.0147       0.0177       0.0192       0.0455       0.0212       0.0259       0.0309       0.0879       0.0146\n",
            "      4    60         0.84         0.84       0.0234       0.0407       0.0254        0.019         0.02       0.0215       0.0446       0.0282       0.0342       0.0356       0.0892       0.0149\n",
            "      4    70         0.65         0.65       0.0216       0.0358       0.0223       0.0207       0.0198       0.0209       0.0382       0.0311       0.0294       0.0329        0.099       0.0165\n",
            "      4    80        0.769        0.769       0.0228       0.0389       0.0247       0.0204       0.0178        0.021        0.043        0.031       0.0267       0.0336       0.0932       0.0155\n",
            "      4    90        0.763        0.763       0.0225       0.0388       0.0239       0.0185       0.0207       0.0211        0.043       0.0277       0.0292       0.0333       0.0907       0.0151\n",
            "      4   100        0.688        0.688       0.0203       0.0368       0.0215        0.016       0.0196       0.0191        0.041       0.0228       0.0296       0.0311       0.0794       0.0132\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      4    10        0.507        0.507       0.0184       0.0316       0.0191       0.0168       0.0174       0.0178       0.0351       0.0223       0.0237        0.027        0.103       0.0172\n",
            "      4    20        0.547        0.547       0.0204       0.0328       0.0212       0.0155       0.0221       0.0196       0.0358       0.0228       0.0286       0.0291       0.0844       0.0141\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               4   84.261    0.005        0.725        0.725       0.0221       0.0378       0.0236       0.0195       0.0191       0.0207       0.0417       0.0285       0.0279       0.0327       0.0925       0.0154\n",
            "! Validation          4   84.261    0.005        0.618        0.618       0.0204       0.0349        0.021       0.0188       0.0192       0.0197       0.0381       0.0269       0.0279       0.0309       0.0926       0.0154\n",
            "Wall time: 84.26351147700007\n",
            "! Best model        4    0.618\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      5    10        0.951        0.951       0.0251       0.0433       0.0269       0.0219       0.0207       0.0232       0.0483       0.0318       0.0303       0.0368       0.0939       0.0156\n",
            "      5    20         0.85         0.85       0.0232       0.0409       0.0259       0.0181       0.0176       0.0205       0.0463       0.0304       0.0235       0.0334       0.0943       0.0157\n",
            "      5    30         1.31         1.31       0.0288       0.0507        0.034       0.0183       0.0185       0.0236       0.0573       0.0386        0.028       0.0413       0.0928       0.0155\n",
            "      5    40         0.63         0.63       0.0223       0.0352       0.0237       0.0186       0.0202       0.0208       0.0385       0.0263       0.0285       0.0311       0.0892       0.0149\n",
            "      5    50        0.789        0.789       0.0258       0.0394       0.0274       0.0246       0.0208       0.0243       0.0419       0.0375         0.03       0.0365       0.0955       0.0159\n",
            "      5    60        0.827        0.827       0.0244       0.0404       0.0267       0.0195       0.0204       0.0222       0.0448       0.0266       0.0323       0.0346        0.085       0.0142\n",
            "      5    70        0.712        0.712       0.0212       0.0375       0.0241       0.0148       0.0156       0.0182       0.0425       0.0244       0.0244       0.0304       0.0851       0.0142\n",
            "      5    80        0.873        0.873       0.0219       0.0415       0.0251       0.0147        0.016       0.0186       0.0476       0.0227       0.0271       0.0325       0.0872       0.0145\n",
            "      5    90         0.64         0.64       0.0218       0.0355       0.0219       0.0231       0.0201       0.0217       0.0375       0.0333        0.029       0.0332       0.0986       0.0164\n",
            "      5   100        0.689        0.689       0.0229       0.0368       0.0247       0.0197       0.0191       0.0211       0.0408       0.0275       0.0271       0.0318        0.104       0.0174\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      5    10        0.496        0.496       0.0181       0.0312        0.019       0.0158       0.0169       0.0172        0.035        0.021       0.0225       0.0262        0.101       0.0169\n",
            "      5    20        0.539        0.539       0.0203       0.0326       0.0213       0.0147        0.022       0.0193       0.0358       0.0213       0.0281       0.0284       0.0829       0.0138\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               5  103.222    0.005        0.717        0.717       0.0219       0.0376       0.0235       0.0189       0.0187       0.0204       0.0417        0.028       0.0272       0.0323       0.0909       0.0151\n",
            "! Validation          5  103.222    0.005        0.611        0.611       0.0203       0.0347       0.0211       0.0183       0.0187       0.0194       0.0381       0.0262       0.0271       0.0304       0.0909       0.0152\n",
            "Wall time: 103.2255057819998\n",
            "! Best model        5    0.611\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      6    10        0.714        0.714       0.0216       0.0375        0.023       0.0194       0.0186       0.0203       0.0415       0.0293       0.0259       0.0323       0.0882       0.0147\n",
            "      6    20        0.632        0.632       0.0198       0.0353         0.02       0.0203       0.0188       0.0197       0.0388       0.0276       0.0263       0.0309       0.0956       0.0159\n",
            "      6    30        0.632        0.632       0.0208       0.0353       0.0223       0.0184        0.017       0.0192       0.0394       0.0254       0.0247       0.0299       0.0838        0.014\n",
            "      6    40        0.841        0.841       0.0235       0.0407       0.0213       0.0309       0.0248       0.0257       0.0355        0.058        0.039       0.0442        0.108       0.0179\n",
            "      6    50         0.74         0.74       0.0222       0.0382       0.0242       0.0163       0.0199       0.0201       0.0428       0.0246       0.0282       0.0319        0.095       0.0158\n",
            "      6    60        0.652        0.652       0.0203       0.0358       0.0215       0.0172       0.0185       0.0191       0.0394       0.0263       0.0285       0.0314       0.0955       0.0159\n",
            "      6    70        0.892        0.892       0.0237       0.0419       0.0274       0.0166       0.0159         0.02       0.0484       0.0244       0.0238       0.0322       0.0875       0.0146\n",
            "      6    80        0.896        0.896       0.0234        0.042       0.0274       0.0143       0.0164       0.0194       0.0487       0.0209       0.0256       0.0317        0.105       0.0175\n",
            "      6    90        0.784        0.784       0.0239       0.0393       0.0249       0.0229       0.0209       0.0229        0.043       0.0312       0.0297       0.0346       0.0733       0.0122\n",
            "      6   100        0.668        0.668       0.0198       0.0363       0.0225       0.0138       0.0151       0.0171        0.042       0.0203       0.0208       0.0277       0.0889       0.0148\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      6    10        0.488        0.488       0.0179        0.031       0.0191       0.0148       0.0163       0.0168        0.035       0.0196       0.0217       0.0254       0.0995       0.0166\n",
            "      6    20        0.535        0.535       0.0201       0.0325       0.0212       0.0145       0.0216       0.0191       0.0358       0.0205       0.0278        0.028       0.0811       0.0135\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               6  122.368    0.005        0.709        0.709       0.0218       0.0374       0.0235       0.0184       0.0181         0.02       0.0417       0.0272       0.0263       0.0317       0.0896       0.0149\n",
            "! Validation          6  122.368    0.005        0.608        0.608       0.0201       0.0346       0.0212       0.0177       0.0185       0.0191       0.0381       0.0257       0.0268       0.0302       0.0891       0.0148\n",
            "Wall time: 122.37071789299989\n",
            "! Best model        6    0.608\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      7    10        0.545        0.545       0.0201       0.0328       0.0201       0.0203       0.0202       0.0202       0.0342       0.0304       0.0289       0.0312       0.0802       0.0134\n",
            "      7    20        0.886        0.886       0.0234       0.0418       0.0264       0.0187       0.0163       0.0205       0.0474       0.0268       0.0279        0.034        0.075       0.0125\n",
            "      7    30        0.615        0.615       0.0215       0.0348        0.023       0.0188       0.0186       0.0201       0.0382       0.0277       0.0255       0.0305       0.0775       0.0129\n",
            "      7    40         0.74         0.74       0.0207       0.0382        0.023       0.0129       0.0194       0.0184       0.0434       0.0196       0.0288       0.0306       0.0849       0.0141\n",
            "      7    50        0.849        0.849       0.0239       0.0409       0.0268       0.0182       0.0177       0.0209       0.0462       0.0291       0.0251       0.0335       0.0919       0.0153\n",
            "      7    60        0.654        0.654        0.021       0.0359       0.0214       0.0213       0.0191       0.0206       0.0389       0.0314       0.0262       0.0322       0.0823       0.0137\n",
            "      7    70        0.734        0.734       0.0212        0.038       0.0236       0.0175       0.0154       0.0188        0.043       0.0257       0.0245       0.0311       0.0917       0.0153\n",
            "      7    80        0.656        0.656       0.0207        0.036        0.023       0.0148       0.0174       0.0184       0.0402       0.0245       0.0263       0.0303       0.0931       0.0155\n",
            "      7    90        0.681        0.681       0.0231       0.0366       0.0244       0.0184       0.0224       0.0217       0.0402       0.0262       0.0302       0.0322        0.106       0.0177\n",
            "      7   100        0.605        0.605       0.0212       0.0345        0.023       0.0188       0.0163       0.0194       0.0385       0.0261        0.023       0.0292       0.0869       0.0145\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      7    10        0.481        0.481       0.0178       0.0308       0.0193       0.0139       0.0158       0.0163        0.035       0.0186       0.0209       0.0248       0.0983       0.0164\n",
            "      7    20        0.531        0.531         0.02       0.0323       0.0213       0.0137       0.0215       0.0188       0.0358       0.0196       0.0275       0.0277       0.0802       0.0134\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               7  141.273    0.005        0.699        0.699       0.0215       0.0371       0.0235       0.0177       0.0174       0.0195       0.0416       0.0263       0.0253       0.0311        0.089       0.0148\n",
            "! Validation          7  141.273    0.005        0.602        0.602         0.02       0.0344       0.0212       0.0171        0.018       0.0188       0.0381        0.025       0.0262       0.0298       0.0879       0.0147\n",
            "Wall time: 141.2747150099999\n",
            "! Best model        7    0.602\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      8    10        0.567        0.567       0.0194       0.0334         0.02       0.0179       0.0189       0.0189       0.0361       0.0271       0.0273       0.0302       0.0839        0.014\n",
            "      8    20         0.81         0.81       0.0237       0.0399       0.0265       0.0187       0.0176       0.0209       0.0457       0.0261       0.0234       0.0317       0.0916       0.0153\n",
            "      8    30        0.572        0.572       0.0208       0.0336       0.0216       0.0172       0.0215       0.0201       0.0365       0.0238       0.0296       0.0299       0.0891       0.0148\n",
            "      8    40        0.573        0.573       0.0209       0.0336       0.0229       0.0169       0.0167       0.0188       0.0378       0.0231       0.0231        0.028       0.0834       0.0139\n",
            "      8    50        0.769        0.769       0.0213       0.0389       0.0236        0.016        0.017       0.0189       0.0448       0.0231       0.0232       0.0304       0.0839        0.014\n",
            "      8    60        0.773        0.773       0.0226        0.039       0.0263        0.017       0.0133       0.0189       0.0451       0.0242       0.0201       0.0298        0.084        0.014\n",
            "      8    70        0.602        0.602       0.0198       0.0344       0.0207       0.0167       0.0192       0.0188       0.0376       0.0248       0.0292       0.0305       0.0941       0.0157\n",
            "      8    80        0.842        0.842       0.0236       0.0407       0.0273       0.0169       0.0153       0.0199        0.047       0.0253       0.0218       0.0313       0.0811       0.0135\n",
            "      8    90        0.887        0.887       0.0235       0.0418       0.0277       0.0149       0.0156       0.0194       0.0488       0.0221       0.0219       0.0309       0.0935       0.0156\n",
            "      8   100        0.624        0.624       0.0214       0.0351        0.024       0.0161       0.0162       0.0188       0.0395       0.0253        0.022       0.0289       0.0761       0.0127\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      8    10        0.474        0.474       0.0176       0.0306       0.0194       0.0132       0.0146       0.0158        0.035       0.0176       0.0197       0.0241       0.0971       0.0162\n",
            "      8    20        0.528        0.528         0.02       0.0323       0.0214       0.0135        0.021       0.0186       0.0358       0.0194       0.0271       0.0274       0.0791       0.0132\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               8  159.336    0.005        0.691        0.691       0.0213       0.0369       0.0235       0.0172       0.0168       0.0192       0.0416       0.0256       0.0244       0.0306       0.0885       0.0147\n",
            "! Validation          8  159.336    0.005        0.597        0.597       0.0198       0.0343       0.0212       0.0167       0.0175       0.0185       0.0381       0.0245       0.0256       0.0294       0.0869       0.0145\n",
            "Wall time: 159.33823969200012\n",
            "! Best model        8    0.597\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      9    10        0.392        0.392       0.0175       0.0278       0.0184       0.0157       0.0153       0.0165       0.0305       0.0205        0.022       0.0244       0.0857       0.0143\n",
            "      9    20        0.554        0.554       0.0188        0.033       0.0201        0.014       0.0182       0.0174       0.0371       0.0208       0.0248       0.0276        0.094       0.0157\n",
            "      9    30        0.597        0.597       0.0208       0.0343       0.0227       0.0183       0.0156       0.0189       0.0376       0.0296       0.0229         0.03       0.0802       0.0134\n",
            "      9    40        0.673        0.673       0.0207       0.0364       0.0227       0.0185       0.0147       0.0187       0.0412       0.0266       0.0216       0.0298        0.081       0.0135\n",
            "      9    50        0.504        0.504       0.0194       0.0315       0.0206       0.0163       0.0175       0.0181       0.0347       0.0229       0.0246       0.0274       0.0823       0.0137\n",
            "      9    60        0.752        0.752       0.0212       0.0385       0.0239       0.0168       0.0147       0.0185       0.0443       0.0243       0.0211       0.0299        0.108        0.018\n",
            "      9    70        0.743        0.743       0.0204       0.0383       0.0241       0.0145       0.0115       0.0167        0.045       0.0202       0.0168       0.0273       0.0906       0.0151\n",
            "      9    80         0.76         0.76        0.022       0.0387       0.0237       0.0183       0.0191       0.0204       0.0432       0.0262       0.0287       0.0327       0.0914       0.0152\n",
            "      9    90        0.638        0.638       0.0197       0.0354       0.0219       0.0151       0.0154       0.0175       0.0407       0.0209        0.022       0.0278        0.106       0.0177\n",
            "      9   100        0.583        0.583        0.019       0.0339       0.0225       0.0106       0.0133       0.0154       0.0392       0.0184       0.0203       0.0259        0.084        0.014\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "      9    10        0.472        0.472       0.0175       0.0305       0.0196       0.0127       0.0144       0.0155       0.0351       0.0171       0.0193       0.0238       0.0966       0.0161\n",
            "      9    20        0.521        0.521       0.0199        0.032       0.0214       0.0138       0.0196       0.0183       0.0358       0.0193       0.0259        0.027       0.0787       0.0131\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train               9  177.248    0.005        0.688        0.688       0.0213       0.0368       0.0235       0.0171       0.0166       0.0191       0.0415       0.0254       0.0241       0.0303       0.0883       0.0147\n",
            "! Validation          9  177.248    0.005        0.594        0.594       0.0197       0.0342       0.0212       0.0165       0.0171       0.0183       0.0381       0.0243       0.0252       0.0292       0.0865       0.0144\n",
            "Wall time: 177.250440109\n",
            "! Best model        9    0.594\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     10    10        0.915        0.915       0.0235       0.0425       0.0292       0.0114       0.0128       0.0178       0.0504       0.0174       0.0186       0.0288        0.098       0.0163\n",
            "     10    20        0.608        0.608       0.0202       0.0346       0.0226       0.0173       0.0136       0.0178       0.0393        0.025       0.0195       0.0279       0.0877       0.0146\n",
            "     10    30        0.826        0.826       0.0226       0.0403       0.0257       0.0169       0.0157       0.0194       0.0468       0.0232       0.0214       0.0305        0.085       0.0142\n",
            "     10    40         0.55         0.55       0.0182       0.0329       0.0207       0.0127       0.0138       0.0157       0.0382       0.0174       0.0189       0.0249       0.0921       0.0153\n",
            "     10    50        0.599        0.599       0.0208       0.0344       0.0221       0.0187       0.0178       0.0195       0.0379       0.0266        0.025       0.0298       0.0872       0.0145\n",
            "     10    60        0.832        0.832       0.0235       0.0405       0.0278       0.0139       0.0162       0.0193       0.0468       0.0226       0.0237        0.031       0.0733       0.0122\n",
            "     10    70        0.561        0.561       0.0189       0.0332       0.0208       0.0143       0.0162       0.0171       0.0375       0.0214       0.0234       0.0274       0.0854       0.0142\n",
            "     10    80        0.669        0.669       0.0216       0.0363       0.0242       0.0178       0.0148       0.0189       0.0411       0.0265       0.0208       0.0295       0.0787       0.0131\n",
            "     10    90        0.741        0.741       0.0208       0.0382       0.0227       0.0158       0.0186        0.019       0.0432       0.0246       0.0262       0.0313       0.0865       0.0144\n",
            "     10   100         1.04         1.04       0.0264       0.0453       0.0299       0.0168       0.0219       0.0229       0.0515       0.0258       0.0321       0.0365       0.0784       0.0131\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     10    10        0.468        0.468       0.0174       0.0303       0.0197       0.0118       0.0139       0.0151       0.0351        0.016       0.0185       0.0232       0.0962        0.016\n",
            "     10    20        0.518        0.518       0.0197       0.0319       0.0215       0.0136       0.0186       0.0179       0.0358       0.0191       0.0252       0.0267       0.0785       0.0131\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              10  195.460    0.005        0.679        0.679        0.021       0.0366       0.0234       0.0162       0.0161       0.0186       0.0415       0.0244       0.0232       0.0297       0.0883       0.0147\n",
            "! Validation         10  195.460    0.005        0.591        0.591       0.0196       0.0341       0.0212       0.0161       0.0167        0.018       0.0381       0.0241       0.0247       0.0289       0.0863       0.0144\n",
            "Wall time: 195.46142642099994\n",
            "! Best model       10    0.591\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     11    10        0.598        0.598       0.0194       0.0343       0.0213        0.015       0.0163       0.0175       0.0386       0.0227       0.0244       0.0286       0.0943       0.0157\n",
            "     11    20        0.864        0.864       0.0213       0.0412       0.0257       0.0139       0.0114        0.017       0.0481       0.0238       0.0192       0.0304        0.118       0.0196\n",
            "     11    30        0.722        0.722       0.0221       0.0377       0.0256       0.0174       0.0127       0.0186       0.0436       0.0249       0.0178       0.0287        0.103       0.0171\n",
            "     11    40        0.678        0.678       0.0208       0.0365       0.0237       0.0153       0.0146       0.0179       0.0421       0.0223       0.0201       0.0282       0.0783       0.0131\n",
            "     11    50        0.569        0.569       0.0191       0.0335       0.0212       0.0145       0.0153        0.017       0.0383       0.0206        0.021       0.0266       0.0958        0.016\n",
            "     11    60        0.724        0.724        0.021       0.0378       0.0241       0.0156       0.0144        0.018       0.0431       0.0242       0.0229       0.0301       0.0954       0.0159\n",
            "     11    70        0.734        0.734       0.0214        0.038        0.025       0.0166       0.0119       0.0178       0.0442       0.0239       0.0172       0.0284       0.0804       0.0134\n",
            "     11    80            1            1       0.0245       0.0444       0.0283       0.0186       0.0153       0.0207       0.0509       0.0305        0.024       0.0351       0.0891       0.0148\n",
            "     11    90        0.708        0.708        0.021       0.0373       0.0237       0.0176       0.0139       0.0184       0.0425       0.0262       0.0209       0.0299       0.0878       0.0146\n",
            "     11   100         0.72         0.72       0.0211       0.0376       0.0234       0.0165       0.0165       0.0188       0.0421       0.0249       0.0284       0.0318       0.0994       0.0166\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     11    10        0.465        0.465       0.0174       0.0303       0.0199       0.0113       0.0138        0.015       0.0351        0.015       0.0183       0.0228       0.0957        0.016\n",
            "     11    20        0.515        0.515       0.0196       0.0318       0.0216       0.0134       0.0178       0.0176       0.0358        0.019       0.0244       0.0264        0.078        0.013\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              11  213.728    0.005        0.673        0.673       0.0208       0.0364       0.0234       0.0158       0.0154       0.0182       0.0415       0.0238       0.0225       0.0292       0.0881       0.0147\n",
            "! Validation         11  213.728    0.005         0.59         0.59       0.0196       0.0341       0.0213       0.0159       0.0164       0.0178       0.0381       0.0238       0.0245       0.0288        0.086       0.0143\n",
            "Wall time: 213.73023704399975\n",
            "! Best model       11    0.590\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     12    10        0.645        0.645       0.0201       0.0356       0.0227       0.0145       0.0153       0.0175        0.041       0.0212       0.0208       0.0277       0.0708       0.0118\n",
            "     12    20        0.608        0.608       0.0208       0.0346       0.0224       0.0189       0.0162       0.0191       0.0379       0.0295       0.0234       0.0303       0.0959        0.016\n",
            "     12    30        0.665        0.665       0.0219       0.0362       0.0246       0.0179       0.0148       0.0191       0.0411       0.0264       0.0205       0.0293       0.0866       0.0144\n",
            "     12    40        0.791        0.791       0.0226       0.0395       0.0258       0.0176       0.0148       0.0194       0.0456       0.0249       0.0201       0.0302       0.0936       0.0156\n",
            "     12    50        0.722        0.722       0.0213       0.0377       0.0234        0.016       0.0181       0.0192       0.0422       0.0242       0.0288       0.0318       0.0785       0.0131\n",
            "     12    60        0.561        0.561       0.0201       0.0332       0.0219       0.0168       0.0164       0.0184       0.0361       0.0252        0.028       0.0298       0.0818       0.0136\n",
            "     12    70        0.636        0.636       0.0208       0.0354       0.0221       0.0192       0.0168       0.0194       0.0399       0.0255       0.0223       0.0292       0.0978       0.0163\n",
            "     12    80        0.509        0.509       0.0188       0.0316         0.02       0.0179        0.015       0.0176       0.0349       0.0255       0.0218       0.0274       0.0767       0.0128\n",
            "     12    90        0.778        0.778       0.0205       0.0392       0.0245       0.0127       0.0123       0.0165        0.046       0.0195       0.0188       0.0281        0.101       0.0169\n",
            "     12   100        0.785        0.785       0.0204       0.0393       0.0233       0.0128       0.0161       0.0174       0.0453       0.0222       0.0238       0.0304       0.0927       0.0154\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     12    10        0.464        0.464       0.0174       0.0302         0.02       0.0111       0.0136       0.0149       0.0352       0.0145        0.018       0.0226       0.0955       0.0159\n",
            "     12    20        0.515        0.515       0.0198       0.0319       0.0218       0.0137       0.0177       0.0177       0.0358       0.0193       0.0242       0.0264       0.0779        0.013\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              12  232.156    0.005        0.668        0.668       0.0207       0.0363       0.0234       0.0155       0.0152        0.018       0.0414       0.0233       0.0222        0.029       0.0879       0.0147\n",
            "! Validation         12  232.156    0.005        0.587        0.587       0.0195        0.034       0.0213       0.0157       0.0162       0.0177       0.0381       0.0235        0.024       0.0285       0.0859       0.0143\n",
            "Wall time: 232.15762719199984\n",
            "! Best model       12    0.587\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     13    10        0.899        0.899       0.0218       0.0421       0.0268        0.011       0.0129       0.0169       0.0499       0.0163       0.0196       0.0286        0.075       0.0125\n",
            "     13    20        0.662        0.662       0.0196       0.0361       0.0223       0.0139       0.0149        0.017       0.0408       0.0216       0.0263       0.0296       0.0943       0.0157\n",
            "     13    30        0.676        0.676       0.0209       0.0365       0.0244       0.0155       0.0122       0.0174       0.0422       0.0225       0.0185       0.0278       0.0976       0.0163\n",
            "     13    40        0.719        0.719       0.0211       0.0376        0.024       0.0157       0.0148       0.0182       0.0428       0.0247       0.0235       0.0303       0.0753       0.0126\n",
            "     13    50        0.633        0.633       0.0196       0.0353       0.0221       0.0136       0.0159       0.0172         0.04       0.0202       0.0259       0.0287       0.0767       0.0128\n",
            "     13    60        0.787        0.787        0.022       0.0394       0.0259       0.0142       0.0142       0.0181       0.0454       0.0225       0.0234       0.0304       0.0998       0.0166\n",
            "     13    70        0.551        0.551       0.0193       0.0329       0.0199       0.0178       0.0184       0.0187       0.0361       0.0256       0.0253        0.029       0.0842        0.014\n",
            "     13    80        0.696        0.696       0.0204        0.037        0.023       0.0136        0.017       0.0179       0.0417       0.0204       0.0294       0.0305       0.0981       0.0164\n",
            "     13    90        0.588        0.588       0.0192        0.034        0.022       0.0129       0.0146       0.0165       0.0395       0.0176       0.0203       0.0258       0.0969       0.0161\n",
            "     13   100         0.78         0.78       0.0212       0.0392       0.0255       0.0129       0.0121       0.0168       0.0461       0.0205       0.0171       0.0279       0.0939       0.0156\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     13    10         0.46         0.46       0.0172       0.0301       0.0199       0.0105       0.0131       0.0145       0.0352       0.0135       0.0176       0.0221       0.0951       0.0158\n",
            "     13    20        0.511        0.511       0.0195       0.0317       0.0217        0.013       0.0175       0.0174       0.0358       0.0187       0.0237       0.0261       0.0775       0.0129\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              13  250.548    0.005        0.663        0.663       0.0205       0.0361       0.0233        0.015       0.0147       0.0177       0.0414       0.0229       0.0215       0.0286       0.0878       0.0146\n",
            "! Validation         13  250.548    0.005        0.586        0.586       0.0195        0.034       0.0213       0.0155       0.0159       0.0176       0.0381       0.0235       0.0238       0.0285       0.0857       0.0143\n",
            "Wall time: 250.54937686199992\n",
            "! Best model       13    0.586\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     14    10        0.483        0.483       0.0183       0.0308       0.0198       0.0161       0.0145       0.0168       0.0345        0.023       0.0201       0.0259        0.114       0.0191\n",
            "     14    20        0.844        0.844       0.0226       0.0408       0.0274        0.014        0.012       0.0178       0.0478       0.0224       0.0182       0.0295       0.0985       0.0164\n",
            "     14    30          1.1          1.1       0.0238       0.0466       0.0305      0.00936       0.0118       0.0172       0.0558       0.0142       0.0198       0.0299       0.0886       0.0148\n",
            "     14    40        0.584        0.584       0.0192       0.0339       0.0207       0.0163       0.0164       0.0178       0.0381       0.0235        0.023       0.0282        0.088       0.0147\n",
            "     14    50        0.723        0.723       0.0223       0.0377        0.025       0.0173       0.0165       0.0196        0.043       0.0241       0.0235       0.0302       0.0926       0.0154\n",
            "     14    60        0.644        0.644       0.0214       0.0356       0.0233       0.0166       0.0186       0.0195       0.0396        0.024       0.0273       0.0303       0.0871       0.0145\n",
            "     14    70        0.443        0.443       0.0173       0.0295       0.0181        0.015       0.0163       0.0165       0.0325        0.022        0.023       0.0258       0.0622       0.0104\n",
            "     14    80        0.881        0.881       0.0224       0.0417       0.0267        0.015       0.0129       0.0182       0.0485        0.025       0.0196        0.031          0.1       0.0167\n",
            "     14    90        0.742        0.742       0.0219       0.0382       0.0249       0.0144       0.0173       0.0189       0.0432       0.0243       0.0266       0.0314       0.0909       0.0152\n",
            "     14   100        0.564        0.564        0.019       0.0333       0.0218       0.0133       0.0137       0.0163       0.0386        0.018         0.02       0.0255       0.0907       0.0151\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     14    10        0.459        0.459       0.0171       0.0301         0.02      0.00982       0.0131       0.0143       0.0352       0.0127       0.0175       0.0218       0.0946       0.0158\n",
            "     14    20        0.509        0.509       0.0196       0.0317       0.0219       0.0131       0.0169       0.0173       0.0358       0.0186       0.0232       0.0259        0.077       0.0128\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              14  269.701    0.005        0.659        0.659       0.0204        0.036       0.0233       0.0149       0.0144       0.0175       0.0413       0.0224       0.0211       0.0283       0.0877       0.0146\n",
            "! Validation         14  269.701    0.005        0.585        0.585       0.0194       0.0339       0.0214       0.0153       0.0157       0.0175       0.0381       0.0234       0.0235       0.0283       0.0854       0.0142\n",
            "Wall time: 269.7050096779999\n",
            "! Best model       14    0.585\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     15    10        0.373        0.373        0.017       0.0271       0.0181       0.0181       0.0115       0.0159       0.0297       0.0249       0.0159       0.0235       0.0914       0.0152\n",
            "     15    20        0.625        0.625       0.0207       0.0351       0.0212         0.02       0.0196       0.0202       0.0372       0.0316       0.0294       0.0327       0.0741       0.0124\n",
            "     15    30        0.814        0.814       0.0236         0.04       0.0281       0.0151       0.0141       0.0191       0.0468       0.0207       0.0209       0.0295       0.0813       0.0135\n",
            "     15    40        0.807        0.807       0.0219       0.0399       0.0265        0.011       0.0143       0.0173       0.0468        0.018       0.0212       0.0287       0.0831       0.0139\n",
            "     15    50        0.544        0.544       0.0187       0.0327       0.0201       0.0156       0.0161       0.0173       0.0367       0.0224       0.0228       0.0273       0.0765       0.0127\n",
            "     15    60        0.409        0.409       0.0168       0.0284       0.0188       0.0122        0.013       0.0147       0.0321       0.0205       0.0172       0.0232       0.0962        0.016\n",
            "     15    70        0.917        0.917       0.0234       0.0425       0.0285       0.0126        0.014       0.0183       0.0497       0.0228       0.0212       0.0312       0.0977       0.0163\n",
            "     15    80        0.562        0.562       0.0191       0.0333       0.0215       0.0147       0.0139       0.0167       0.0382       0.0217       0.0181        0.026       0.0695       0.0116\n",
            "     15    90        0.776        0.776       0.0219       0.0391       0.0249       0.0162       0.0157       0.0189       0.0449       0.0247       0.0227       0.0308        0.101       0.0169\n",
            "     15   100        0.792        0.792       0.0224       0.0395       0.0263        0.016       0.0132       0.0185       0.0459       0.0237        0.019       0.0295       0.0837       0.0139\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     15    10        0.456        0.456        0.017         0.03         0.02      0.00967       0.0126       0.0141       0.0351       0.0124       0.0172       0.0216       0.0943       0.0157\n",
            "     15    20        0.506        0.506       0.0195       0.0316       0.0218       0.0132       0.0166       0.0172       0.0358       0.0183       0.0229       0.0257       0.0767       0.0128\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              15  289.306    0.005        0.654        0.654       0.0202       0.0359       0.0232       0.0144       0.0141       0.0172       0.0413       0.0219       0.0206        0.028       0.0876       0.0146\n",
            "! Validation         15  289.306    0.005        0.585        0.585       0.0194       0.0339       0.0214       0.0153       0.0154       0.0174       0.0381       0.0235       0.0234       0.0283       0.0852       0.0142\n",
            "Wall time: 289.3093893719997\n",
            "! Best model       15    0.585\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     16    10        0.578        0.578       0.0197       0.0337        0.022       0.0172       0.0128       0.0173       0.0383        0.024       0.0196       0.0273       0.0881       0.0147\n",
            "     16    20        0.696        0.696       0.0208        0.037        0.025       0.0128       0.0122       0.0167       0.0435       0.0184       0.0173       0.0264       0.0958        0.016\n",
            "     16    30        0.418        0.418       0.0174       0.0287       0.0188       0.0156       0.0137        0.016       0.0318       0.0212       0.0211       0.0247       0.0872       0.0145\n",
            "     16    40        0.654        0.654       0.0209       0.0359       0.0237       0.0176       0.0132       0.0182        0.041       0.0257       0.0188       0.0285       0.0756       0.0126\n",
            "     16    50        0.605        0.605       0.0187       0.0345       0.0215       0.0125       0.0135       0.0158       0.0405       0.0161       0.0179       0.0248       0.0719        0.012\n",
            "     16    60        0.521        0.521       0.0186        0.032       0.0206       0.0163       0.0128       0.0166       0.0364       0.0229       0.0177       0.0257       0.0812       0.0135\n",
            "     16    70        0.831        0.831       0.0227       0.0405       0.0272       0.0135       0.0139       0.0182       0.0475       0.0205       0.0197       0.0292       0.0914       0.0152\n",
            "     16    80         0.87         0.87       0.0236       0.0414       0.0288       0.0132       0.0134       0.0185       0.0487       0.0198       0.0203       0.0296       0.0812       0.0135\n",
            "     16    90        0.786        0.786       0.0215       0.0393       0.0258       0.0126       0.0132       0.0172       0.0462       0.0194        0.019       0.0282       0.0889       0.0148\n",
            "     16   100        0.615        0.615       0.0182       0.0348       0.0215       0.0105       0.0125       0.0148       0.0409        0.015       0.0192        0.025       0.0864       0.0144\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     16    10        0.454        0.454       0.0169       0.0299         0.02      0.00938       0.0122       0.0138       0.0351       0.0119       0.0167       0.0212        0.094       0.0157\n",
            "     16    20        0.506        0.506       0.0196       0.0316       0.0219       0.0132       0.0164       0.0172       0.0358       0.0184       0.0227       0.0256       0.0762       0.0127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              16  308.396    0.005         0.65         0.65       0.0201       0.0358       0.0231       0.0141       0.0138        0.017       0.0412       0.0215       0.0203       0.0277       0.0877       0.0146\n",
            "! Validation         16  308.396    0.005        0.585        0.585       0.0194       0.0339       0.0215       0.0152       0.0152       0.0173       0.0381       0.0234       0.0233       0.0283       0.0851       0.0142\n",
            "Wall time: 308.3974410420001\n",
            "! Best model       16    0.585\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     17    10         0.83         0.83       0.0213       0.0404       0.0259       0.0111       0.0132       0.0167       0.0475       0.0195       0.0203       0.0291        0.094       0.0157\n",
            "     17    20        0.403        0.403       0.0164       0.0282       0.0179       0.0145       0.0125        0.015       0.0312       0.0222       0.0195       0.0243         0.11       0.0183\n",
            "     17    30        0.947        0.947       0.0242       0.0432       0.0296       0.0127       0.0144       0.0189        0.051       0.0185       0.0213       0.0302       0.0794       0.0132\n",
            "     17    40        0.524        0.524       0.0183       0.0321       0.0205        0.012       0.0156       0.0161       0.0368       0.0171       0.0219       0.0253        0.086       0.0143\n",
            "     17    50        0.559        0.559       0.0183       0.0332       0.0214       0.0115       0.0129       0.0153       0.0388       0.0168       0.0172       0.0243          0.1       0.0167\n",
            "     17    60        0.528        0.528       0.0194       0.0322       0.0217       0.0138       0.0155        0.017       0.0368       0.0197       0.0208       0.0258       0.0858       0.0143\n",
            "     17    70        0.797        0.797       0.0209       0.0396       0.0244        0.013       0.0153       0.0175        0.046       0.0203       0.0234       0.0299       0.0911       0.0152\n",
            "     17    80        0.665        0.665       0.0207       0.0362       0.0239       0.0138       0.0148       0.0175       0.0417       0.0205       0.0218        0.028       0.0921       0.0154\n",
            "     17    90        0.723        0.723       0.0213       0.0377       0.0246       0.0134       0.0158       0.0179       0.0439       0.0204       0.0208       0.0284       0.0778        0.013\n",
            "     17   100        0.512        0.512       0.0179       0.0318       0.0208       0.0117       0.0123       0.0149       0.0363       0.0197       0.0199       0.0253       0.0701       0.0117\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     17    10         0.45         0.45       0.0168       0.0298         0.02      0.00896       0.0117       0.0135       0.0351       0.0115        0.016       0.0209       0.0942       0.0157\n",
            "     17    20        0.517        0.517       0.0199       0.0319       0.0221       0.0136       0.0172       0.0176        0.036       0.0192       0.0238       0.0263       0.0763       0.0127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              17  326.605    0.005        0.646        0.646         0.02       0.0357       0.0231       0.0137       0.0136       0.0168       0.0412       0.0211       0.0199       0.0274       0.0878       0.0146\n",
            "! Validation         17  326.605    0.005        0.584        0.584       0.0193       0.0339       0.0215        0.015       0.0151       0.0172       0.0382       0.0232       0.0231       0.0282       0.0852       0.0142\n",
            "Wall time: 326.6069612319998\n",
            "! Best model       17    0.584\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     18    10         0.68         0.68       0.0204       0.0366       0.0225       0.0174       0.0149       0.0182       0.0402       0.0326       0.0225       0.0318        0.096        0.016\n",
            "     18    20        0.802        0.802       0.0222       0.0397       0.0273       0.0121       0.0119       0.0171       0.0467       0.0207       0.0178       0.0284       0.0895       0.0149\n",
            "     18    30        0.665        0.665       0.0207       0.0362       0.0243       0.0143       0.0128       0.0171       0.0418       0.0207       0.0205       0.0277       0.0681       0.0113\n",
            "     18    40        0.619        0.619       0.0181       0.0349       0.0216       0.0121       0.0104       0.0147       0.0411       0.0187       0.0147       0.0248        0.102       0.0171\n",
            "     18    50         0.81         0.81       0.0208       0.0399       0.0258      0.00978       0.0121       0.0159       0.0474       0.0146       0.0191        0.027       0.0814       0.0136\n",
            "     18    60        0.548        0.548       0.0195       0.0328       0.0221       0.0167       0.0119       0.0169        0.037       0.0259       0.0178       0.0269       0.0628       0.0105\n",
            "     18    70        0.705        0.705       0.0209       0.0373       0.0247       0.0134       0.0133       0.0172       0.0435       0.0194       0.0192       0.0274       0.0818       0.0136\n",
            "     18    80        0.744        0.744        0.022       0.0383        0.025       0.0165       0.0158       0.0191       0.0442       0.0232       0.0208       0.0294       0.0822       0.0137\n",
            "     18    90         0.62         0.62       0.0194       0.0349       0.0224       0.0134       0.0135       0.0164       0.0403       0.0214       0.0193        0.027       0.0877       0.0146\n",
            "     18   100        0.608        0.608       0.0184       0.0346       0.0217       0.0104       0.0129        0.015       0.0407       0.0148       0.0183       0.0246       0.0793       0.0132\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     18    10        0.449        0.449       0.0168       0.0298       0.0201      0.00873       0.0117       0.0135       0.0351       0.0115       0.0158       0.0208       0.0943       0.0157\n",
            "     18    20        0.515        0.515       0.0196       0.0318       0.0222       0.0129       0.0163       0.0171        0.036       0.0189       0.0233       0.0261       0.0763       0.0127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              18  344.909    0.005        0.643        0.643       0.0199       0.0356        0.023       0.0137       0.0133       0.0167       0.0412        0.021       0.0196       0.0272       0.0879       0.0146\n",
            "! Validation         18  344.909    0.005        0.582        0.582       0.0193       0.0339       0.0215       0.0148       0.0149       0.0171       0.0382       0.0229       0.0231       0.0281       0.0853       0.0142\n",
            "Wall time: 344.9106533280001\n",
            "! Best model       18    0.582\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     19    10          0.5          0.5       0.0183       0.0314       0.0202        0.015        0.014       0.0164       0.0359       0.0198       0.0191       0.0249        0.102        0.017\n",
            "     19    20        0.698        0.698       0.0202       0.0371       0.0242        0.012       0.0121       0.0161       0.0436        0.019       0.0166       0.0264        0.108       0.0181\n",
            "     19    30        0.827        0.827       0.0222       0.0404       0.0271       0.0127       0.0117       0.0172       0.0477       0.0197       0.0174       0.0282       0.0676       0.0113\n",
            "     19    40        0.822        0.822       0.0202       0.0402       0.0244       0.0144      0.00901       0.0159       0.0473       0.0237       0.0144       0.0285       0.0929       0.0155\n",
            "     19    50        0.698        0.698       0.0202       0.0371       0.0244       0.0114       0.0125       0.0161       0.0437       0.0166        0.018       0.0261       0.0821       0.0137\n",
            "     19    60        0.397        0.397       0.0176        0.028       0.0189       0.0146       0.0151       0.0162       0.0312       0.0195       0.0201       0.0236       0.0908       0.0151\n",
            "     19    70        0.957        0.957        0.024       0.0434         0.03       0.0109       0.0128       0.0179       0.0516       0.0159       0.0197       0.0291       0.0997       0.0166\n",
            "     19    80        0.439        0.439       0.0175       0.0294       0.0187        0.015       0.0155       0.0164       0.0329       0.0212       0.0205       0.0248       0.0975       0.0163\n",
            "     19    90        0.605        0.605       0.0202       0.0345       0.0231        0.016       0.0131       0.0174       0.0397       0.0223       0.0183       0.0268       0.0849       0.0142\n",
            "     19   100        0.712        0.712       0.0198       0.0375       0.0234       0.0114       0.0135       0.0161       0.0437        0.019       0.0202       0.0276       0.0777        0.013\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     19    10        0.446        0.446       0.0166       0.0297         0.02       0.0084       0.0113       0.0132       0.0351       0.0111       0.0154       0.0205       0.0945       0.0158\n",
            "     19    20        0.513        0.513       0.0197       0.0318       0.0223       0.0122       0.0165        0.017        0.036       0.0176       0.0235       0.0257       0.0766       0.0128\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              19  363.213    0.005         0.64         0.64       0.0198       0.0355       0.0231       0.0133       0.0131       0.0165       0.0411       0.0204       0.0192       0.0269       0.0881       0.0147\n",
            "! Validation         19  363.213    0.005        0.581        0.581       0.0192       0.0338       0.0215       0.0146       0.0148        0.017       0.0381       0.0228       0.0228       0.0279       0.0856       0.0143\n",
            "Wall time: 363.2143633830001\n",
            "! Best model       19    0.581\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     20    10        0.457        0.457       0.0175         0.03         0.02       0.0111       0.0135       0.0149       0.0344       0.0165       0.0194       0.0234       0.0762       0.0127\n",
            "     20    20        0.711        0.711       0.0199       0.0374       0.0241       0.0108       0.0124       0.0158       0.0441       0.0182       0.0176       0.0266       0.0898        0.015\n",
            "     20    30         0.64         0.64       0.0188       0.0355       0.0225       0.0115       0.0112       0.0151        0.042       0.0167       0.0149       0.0246       0.0871       0.0145\n",
            "     20    40        0.524        0.524       0.0179       0.0321       0.0211        0.013       0.0104       0.0148       0.0375        0.018       0.0156       0.0237       0.0959        0.016\n",
            "     20    50        0.708        0.708       0.0203       0.0374       0.0237       0.0145       0.0121       0.0168       0.0433       0.0235       0.0181       0.0283       0.0814       0.0136\n",
            "     20    60        0.871        0.871       0.0217       0.0414       0.0261       0.0124       0.0135       0.0173       0.0487       0.0219       0.0183       0.0296       0.0968       0.0161\n",
            "     20    70        0.572        0.572       0.0214       0.0336       0.0239        0.018        0.015        0.019       0.0376       0.0261       0.0203        0.028       0.0741       0.0123\n",
            "     20    80        0.745        0.745       0.0212       0.0383       0.0255       0.0122       0.0127       0.0168       0.0451       0.0188       0.0173       0.0271       0.0945       0.0158\n",
            "     20    90        0.469        0.469       0.0182       0.0304       0.0179       0.0209       0.0165       0.0184       0.0317        0.029        0.026       0.0289       0.0604       0.0101\n",
            "     20   100        0.467        0.467       0.0179       0.0303       0.0196       0.0133       0.0154       0.0161       0.0339       0.0204       0.0227       0.0257       0.0764       0.0127\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     20    10        0.445        0.445       0.0166       0.0296         0.02      0.00881       0.0107       0.0132        0.035       0.0112       0.0149       0.0204       0.0946       0.0158\n",
            "     20    20         0.52         0.52       0.0198        0.032       0.0226       0.0121       0.0165       0.0171       0.0362        0.018       0.0238        0.026       0.0764       0.0127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              20  381.723    0.005        0.635        0.635       0.0196       0.0354        0.023        0.013       0.0127       0.0162       0.0411         0.02       0.0188       0.0266       0.0882       0.0147\n",
            "! Validation         20  381.723    0.005        0.582        0.582       0.0192       0.0338       0.0215       0.0145       0.0146       0.0169       0.0382       0.0228       0.0229        0.028       0.0857       0.0143\n",
            "Wall time: 381.72433700700003\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     21    10        0.661        0.661       0.0206       0.0361       0.0241       0.0145       0.0128       0.0171       0.0419       0.0219       0.0177       0.0272       0.0766       0.0128\n",
            "     21    20        0.738        0.738       0.0208       0.0381       0.0244       0.0148       0.0124       0.0172       0.0442       0.0224       0.0203        0.029       0.0981       0.0163\n",
            "     21    30        0.766        0.766       0.0197       0.0388        0.025      0.00848      0.00959       0.0144       0.0465        0.014        0.014       0.0248        0.108        0.018\n",
            "     21    40        0.733        0.733       0.0208        0.038       0.0252        0.013       0.0108       0.0163       0.0449       0.0191       0.0149       0.0263          0.1       0.0167\n",
            "     21    50        0.694        0.694       0.0203        0.037       0.0237       0.0136       0.0136       0.0169       0.0422       0.0226        0.024       0.0296       0.0913       0.0152\n",
            "     21    60        0.795        0.795       0.0209       0.0396       0.0253       0.0123       0.0116       0.0164       0.0465       0.0198       0.0189       0.0284       0.0911       0.0152\n",
            "     21    70        0.617        0.617        0.019       0.0349       0.0223       0.0139       0.0112       0.0158       0.0404        0.022       0.0169       0.0264       0.0813       0.0135\n",
            "     21    80        0.971        0.971       0.0252       0.0437       0.0303       0.0153       0.0149       0.0202       0.0511       0.0221       0.0234       0.0322        0.087       0.0145\n",
            "     21    90        0.862        0.862       0.0216       0.0412       0.0268       0.0107       0.0118       0.0164        0.049       0.0154       0.0183       0.0276        0.102        0.017\n",
            "     21   100        0.885        0.885       0.0219       0.0417       0.0272       0.0117       0.0113       0.0167       0.0496       0.0184       0.0162       0.0281        0.103       0.0172\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     21    10        0.445        0.445       0.0166       0.0296       0.0199      0.00896       0.0113       0.0134        0.035       0.0116       0.0152       0.0206       0.0944       0.0157\n",
            "     21    20        0.517        0.517       0.0197       0.0319       0.0226       0.0122       0.0158       0.0169       0.0362       0.0177       0.0234       0.0258        0.076       0.0127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              21  400.128    0.005        0.633        0.633       0.0195       0.0353       0.0229        0.013       0.0127       0.0162        0.041       0.0197       0.0188       0.0265       0.0877       0.0146\n",
            "! Validation         21  400.128    0.005        0.582        0.582       0.0192       0.0339       0.0215       0.0146       0.0147       0.0169       0.0382        0.023       0.0229        0.028       0.0855       0.0142\n",
            "Wall time: 400.1294090020001\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     22    10        0.532        0.532        0.018       0.0324       0.0205       0.0133       0.0125       0.0154       0.0373       0.0204       0.0175       0.0251       0.0707       0.0118\n",
            "     22    20         0.99         0.99       0.0229       0.0442       0.0291       0.0103       0.0109       0.0168       0.0528       0.0154       0.0171       0.0285       0.0978       0.0163\n",
            "     22    30        0.592        0.592       0.0197       0.0341       0.0237       0.0126       0.0108       0.0157       0.0401       0.0179       0.0152       0.0244       0.0852       0.0142\n",
            "     22    40        0.602        0.602         0.02       0.0344        0.022       0.0174       0.0146        0.018       0.0387        0.026       0.0209       0.0286       0.0706       0.0118\n",
            "     22    50        0.589        0.589       0.0194       0.0341       0.0227       0.0105        0.015       0.0161       0.0397       0.0146       0.0209       0.0251       0.0842        0.014\n",
            "     22    60        0.604        0.604       0.0189       0.0345       0.0232       0.0121      0.00853       0.0146       0.0407       0.0177       0.0135        0.024        0.097       0.0162\n",
            "     22    70        0.518        0.518       0.0182        0.032       0.0213       0.0127       0.0116       0.0152       0.0369       0.0185       0.0184       0.0246       0.0833       0.0139\n",
            "     22    80        0.485        0.485       0.0174       0.0309       0.0181       0.0183       0.0138       0.0167       0.0309       0.0382       0.0214       0.0302        0.109       0.0181\n",
            "     22    90        0.488        0.488       0.0178        0.031       0.0201       0.0131       0.0135       0.0156       0.0354        0.018       0.0208       0.0247        0.081       0.0135\n",
            "     22   100        0.774        0.774       0.0225        0.039       0.0267       0.0133       0.0149       0.0183       0.0456         0.02       0.0201       0.0286       0.0827       0.0138\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     22    10        0.444        0.444       0.0166       0.0296       0.0199        0.009       0.0111       0.0133       0.0349       0.0117       0.0149       0.0205       0.0942       0.0157\n",
            "     22    20        0.522        0.522       0.0199       0.0321       0.0227       0.0123       0.0165       0.0172       0.0363       0.0176       0.0241        0.026       0.0761       0.0127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              22  418.644    0.005        0.631        0.631       0.0195       0.0353       0.0229       0.0127       0.0124        0.016        0.041       0.0196       0.0185       0.0264       0.0881       0.0147\n",
            "! Validation         22  418.644    0.005        0.582        0.582       0.0192       0.0339       0.0215       0.0145       0.0146       0.0169       0.0382       0.0228       0.0229        0.028       0.0856       0.0143\n",
            "Wall time: 418.6466762760001\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     23    10        0.646        0.646       0.0185       0.0357       0.0226       0.0115      0.00891       0.0143       0.0424       0.0167       0.0127        0.024       0.0936       0.0156\n",
            "     23    20        0.848        0.848       0.0218       0.0409       0.0262       0.0139       0.0119       0.0173       0.0481       0.0217       0.0168       0.0289        0.105       0.0175\n",
            "     23    30        0.472        0.472       0.0178       0.0305       0.0195       0.0153        0.013        0.016       0.0344       0.0216       0.0197       0.0252       0.0627       0.0104\n",
            "     23    40        0.521        0.521       0.0195        0.032       0.0221       0.0138       0.0147       0.0169       0.0366       0.0196       0.0203       0.0255       0.0761       0.0127\n",
            "     23    50        0.652        0.652       0.0188       0.0358       0.0217       0.0135       0.0124       0.0159       0.0416       0.0207       0.0183       0.0269       0.0768       0.0128\n",
            "     23    60          0.6          0.6       0.0204       0.0344       0.0231       0.0154       0.0148       0.0178       0.0392       0.0228       0.0207       0.0276       0.0775       0.0129\n",
            "     23    70        0.416        0.416       0.0164       0.0286       0.0194       0.0109         0.01       0.0134       0.0334       0.0159       0.0138       0.0211       0.0824       0.0137\n",
            "     23    80        0.715        0.715       0.0204       0.0375       0.0245       0.0113       0.0129       0.0162       0.0442       0.0166       0.0192       0.0267        0.096        0.016\n",
            "     23    90        0.561        0.561       0.0197       0.0332       0.0223       0.0142       0.0149       0.0171        0.038       0.0202       0.0209       0.0264       0.0682       0.0114\n",
            "     23   100        0.737        0.737       0.0207       0.0381       0.0239        0.014       0.0143       0.0174       0.0442       0.0213       0.0213       0.0289       0.0958        0.016\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     23    10        0.442        0.442       0.0165       0.0295         0.02      0.00887       0.0105       0.0131       0.0349       0.0117       0.0141       0.0202       0.0942       0.0157\n",
            "     23    20         0.53         0.53       0.0202       0.0323        0.023       0.0122       0.0167       0.0173       0.0366       0.0176       0.0243       0.0262        0.076       0.0127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              23  437.826    0.005        0.629        0.629       0.0194       0.0352       0.0228       0.0126       0.0123       0.0159        0.041       0.0193       0.0183       0.0262       0.0879       0.0147\n",
            "! Validation         23  437.826    0.005        0.582        0.582       0.0192       0.0338       0.0216       0.0145       0.0146       0.0169       0.0382       0.0228       0.0229        0.028       0.0855       0.0143\n",
            "Wall time: 437.8285237269997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     24    10        0.668        0.668       0.0199       0.0363       0.0231       0.0134       0.0135       0.0167       0.0418       0.0208       0.0217       0.0281       0.0835       0.0139\n",
            "     24    20        0.676        0.676       0.0196       0.0365        0.023       0.0129       0.0128       0.0162       0.0428        0.019       0.0174       0.0264          0.1       0.0167\n",
            "     24    30         0.65         0.65       0.0192       0.0358       0.0231       0.0107       0.0122       0.0153       0.0422       0.0158       0.0178       0.0252       0.0941       0.0157\n",
            "     24    40        0.564        0.564       0.0189       0.0333       0.0222        0.012       0.0127       0.0157       0.0386       0.0207        0.017       0.0254       0.0849       0.0142\n",
            "     24    50        0.575        0.575       0.0188       0.0336       0.0222       0.0124       0.0117       0.0154       0.0393       0.0187        0.016       0.0247        0.078        0.013\n",
            "     24    60        0.468        0.468       0.0171       0.0304         0.02       0.0104       0.0121       0.0142       0.0356       0.0151       0.0154        0.022       0.0945       0.0157\n",
            "     24    70        0.704        0.704       0.0204       0.0372       0.0252       0.0113         0.01       0.0155       0.0443       0.0169        0.014        0.025       0.0879       0.0147\n",
            "     24    80        0.993        0.993       0.0232       0.0442       0.0294       0.0112         0.01       0.0169       0.0528       0.0185       0.0159       0.0291          0.1       0.0167\n",
            "     24    90        0.598        0.598       0.0191       0.0343       0.0215       0.0139       0.0146       0.0167        0.039       0.0223       0.0216       0.0277       0.0813       0.0136\n",
            "     24   100        0.894        0.894       0.0235        0.042       0.0292       0.0127       0.0113       0.0177       0.0494       0.0212       0.0187       0.0297        0.091       0.0152\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     24    10        0.441        0.441       0.0165       0.0295         0.02      0.00889       0.0103       0.0131       0.0349       0.0118       0.0139       0.0202       0.0943       0.0157\n",
            "     24    20        0.524        0.524         0.02       0.0321       0.0229       0.0118       0.0165       0.0171       0.0365       0.0163       0.0244       0.0257       0.0761       0.0127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              24  456.624    0.005        0.623        0.623       0.0192        0.035       0.0227       0.0121       0.0119       0.0156       0.0409       0.0187       0.0177       0.0258       0.0881       0.0147\n",
            "! Validation         24  456.624    0.005        0.579        0.579       0.0191       0.0338       0.0216       0.0143       0.0143       0.0167       0.0382       0.0225       0.0227       0.0278       0.0856       0.0143\n",
            "Wall time: 456.6268952380001\n",
            "! Best model       24    0.579\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     25    10        0.704        0.704       0.0199       0.0372       0.0235       0.0125       0.0129       0.0163       0.0436       0.0196       0.0178        0.027       0.0762       0.0127\n",
            "     25    20        0.536        0.536       0.0179       0.0325       0.0211       0.0108       0.0121       0.0147        0.038       0.0155       0.0175       0.0237       0.0991       0.0165\n",
            "     25    30        0.788        0.788       0.0214       0.0394       0.0274      0.00947      0.00918       0.0154       0.0471       0.0149       0.0143       0.0254       0.0922       0.0154\n",
            "     25    40        0.378        0.378       0.0153       0.0273       0.0168       0.0123       0.0122       0.0138       0.0309       0.0198       0.0159       0.0222       0.0957        0.016\n",
            "     25    50        0.621        0.621        0.021        0.035       0.0232       0.0167       0.0163       0.0188       0.0396       0.0224        0.024       0.0287       0.0745       0.0124\n",
            "     25    60        0.612        0.612       0.0187       0.0347       0.0228       0.0107       0.0103       0.0146        0.041       0.0173       0.0143       0.0242       0.0929       0.0155\n",
            "     25    70        0.599        0.599       0.0194       0.0343       0.0232       0.0111       0.0124       0.0156       0.0404       0.0156       0.0172       0.0244       0.0904       0.0151\n",
            "     25    80        0.561        0.561        0.019       0.0332       0.0221       0.0131       0.0126       0.0159       0.0386       0.0185       0.0177       0.0249       0.0878       0.0146\n",
            "     25    90        0.486        0.486       0.0171       0.0309       0.0205       0.0102       0.0102       0.0136       0.0364       0.0155       0.0144       0.0221       0.0914       0.0152\n",
            "     25   100         0.65         0.65       0.0202       0.0358       0.0236       0.0132       0.0135       0.0168       0.0415        0.021       0.0183       0.0269       0.0869       0.0145\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     25    10         0.44         0.44       0.0164       0.0294       0.0199      0.00859       0.0105        0.013       0.0349       0.0115       0.0139       0.0201       0.0943       0.0157\n",
            "     25    20        0.528        0.528         0.02       0.0322        0.023       0.0117       0.0163        0.017       0.0367       0.0164       0.0243       0.0258       0.0759       0.0127\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              25  475.234    0.005        0.625        0.625       0.0192       0.0351       0.0228       0.0125       0.0119       0.0157       0.0409       0.0191       0.0177       0.0259       0.0881       0.0147\n",
            "! Validation         25  475.234    0.005        0.579        0.579       0.0191       0.0338       0.0215       0.0143       0.0143       0.0167       0.0382       0.0225       0.0225       0.0277       0.0856       0.0143\n",
            "Wall time: 475.2358554990001\n",
            "! Best model       25    0.579\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     26    10        0.605        0.605        0.018       0.0345       0.0214       0.0116       0.0112       0.0147       0.0405       0.0187       0.0154       0.0249       0.0902        0.015\n",
            "     26    20        0.525        0.525       0.0194       0.0321       0.0223       0.0121       0.0148       0.0164       0.0369       0.0173       0.0212       0.0251       0.0739       0.0123\n",
            "     26    30        0.677        0.677       0.0188       0.0365       0.0226       0.0108       0.0114       0.0149       0.0431       0.0163       0.0177       0.0257       0.0981       0.0163\n",
            "     26    40        0.639        0.639       0.0183       0.0355        0.022       0.0104       0.0116       0.0146       0.0418       0.0158        0.018       0.0252        0.097       0.0162\n",
            "     26    50        0.619        0.619        0.018       0.0349       0.0219      0.00988       0.0104        0.014       0.0413        0.014       0.0173       0.0242        0.102        0.017\n",
            "     26    60        0.734        0.734        0.021        0.038       0.0268      0.00982      0.00942       0.0153       0.0454       0.0159       0.0136        0.025        0.105       0.0176\n",
            "     26    70        0.749        0.749       0.0199       0.0384        0.025      0.00875       0.0106       0.0148       0.0459       0.0128       0.0157       0.0248       0.0991       0.0165\n",
            "     26    80        0.676        0.676       0.0202       0.0365       0.0251       0.0106       0.0102       0.0153       0.0433       0.0167       0.0148       0.0249       0.0864       0.0144\n",
            "     26    90        0.788        0.788       0.0205       0.0394       0.0255       0.0103       0.0108       0.0155       0.0468       0.0154       0.0173       0.0265       0.0935       0.0156\n",
            "     26   100        0.784        0.784       0.0218       0.0393       0.0259       0.0141       0.0129       0.0176        0.046        0.021       0.0188       0.0286       0.0888       0.0148\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     26    10        0.437        0.437       0.0162       0.0293       0.0198      0.00846      0.00974       0.0127       0.0349       0.0114       0.0131       0.0198        0.094       0.0157\n",
            "     26    20        0.536        0.536       0.0201       0.0325       0.0232        0.012       0.0161       0.0171       0.0369        0.017       0.0244       0.0261       0.0757       0.0126\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              26  493.358    0.005        0.618        0.618        0.019       0.0349       0.0227       0.0118       0.0114       0.0153       0.0409       0.0183        0.017       0.0254       0.0881       0.0147\n",
            "! Validation         26  493.358    0.005        0.578        0.578       0.0191       0.0337       0.0216       0.0141        0.014       0.0166       0.0382       0.0224       0.0223       0.0276       0.0855       0.0143\n",
            "Wall time: 493.36019640799987\n",
            "! Best model       26    0.578\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     27    10        0.411        0.411       0.0166       0.0285       0.0191       0.0133         0.01       0.0141       0.0319       0.0231       0.0157       0.0236        0.071       0.0118\n",
            "     27    20        0.751        0.751       0.0207       0.0385        0.024       0.0137       0.0142       0.0173       0.0442       0.0226       0.0233         0.03        0.106       0.0176\n",
            "     27    30        0.716        0.716       0.0201       0.0376       0.0249       0.0131      0.00782       0.0153       0.0437       0.0252       0.0136       0.0275        0.103       0.0172\n",
            "     27    40         0.72         0.72       0.0213       0.0377        0.026       0.0114       0.0122       0.0166       0.0444       0.0182        0.017       0.0265       0.0933       0.0156\n",
            "     27    50        0.688        0.688       0.0184       0.0368       0.0231      0.00923      0.00884       0.0137       0.0442       0.0125       0.0125       0.0231       0.0931       0.0155\n",
            "     27    60        0.506        0.506       0.0176       0.0316       0.0197       0.0143       0.0123       0.0155       0.0363       0.0211       0.0163       0.0246       0.0994       0.0166\n",
            "     27    70        0.473        0.473       0.0168       0.0305       0.0185       0.0129       0.0136        0.015       0.0348       0.0192       0.0191       0.0244       0.0804       0.0134\n",
            "     27    80        0.687        0.687       0.0192       0.0368       0.0228       0.0121       0.0122       0.0157       0.0425        0.023       0.0194       0.0283       0.0923       0.0154\n",
            "     27    90        0.631        0.631       0.0194       0.0352       0.0241      0.00979         0.01       0.0146       0.0419       0.0144        0.015       0.0238       0.0816       0.0136\n",
            "     27   100        0.697        0.697       0.0186        0.037       0.0226       0.0104       0.0106       0.0145       0.0439       0.0163       0.0163       0.0255       0.0995       0.0166\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     27    10        0.438        0.438       0.0162       0.0294       0.0198      0.00828      0.00994       0.0127       0.0349       0.0113       0.0134       0.0199       0.0939       0.0157\n",
            "     27    20         0.54         0.54         0.02       0.0326       0.0233       0.0112       0.0155       0.0167       0.0371       0.0169        0.024        0.026       0.0753       0.0126\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              27  511.528    0.005        0.618        0.618        0.019       0.0349       0.0226       0.0118       0.0115       0.0153       0.0409       0.0182       0.0172       0.0254       0.0881       0.0147\n",
            "! Validation         27  511.528    0.005         0.58         0.58       0.0191       0.0338       0.0216       0.0142       0.0141       0.0166       0.0382       0.0226       0.0224       0.0277       0.0854       0.0142\n",
            "Wall time: 511.52949143099977\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     28    10        0.488        0.488        0.019        0.031       0.0225        0.012       0.0119       0.0155       0.0358        0.018       0.0178       0.0239       0.0711       0.0119\n",
            "     28    20          0.7          0.7       0.0186       0.0371       0.0225       0.0118      0.00989       0.0147        0.044       0.0172       0.0151       0.0254       0.0831       0.0138\n",
            "     28    30        0.761        0.761       0.0206       0.0387        0.026         0.01      0.00959       0.0152       0.0463       0.0147       0.0136       0.0249       0.0914       0.0152\n",
            "     28    40        0.534        0.534       0.0165       0.0324       0.0192       0.0116       0.0109       0.0139        0.038       0.0173       0.0159       0.0237       0.0899        0.015\n",
            "     28    50        0.552        0.552       0.0186        0.033        0.022       0.0124       0.0111       0.0152       0.0383       0.0196       0.0165       0.0248       0.0848       0.0141\n",
            "     28    60         0.63         0.63       0.0191       0.0352       0.0227       0.0131        0.011       0.0156       0.0412       0.0202       0.0154       0.0256       0.0834       0.0139\n",
            "     28    70        0.352        0.352       0.0163       0.0263        0.017       0.0159       0.0137       0.0156       0.0285       0.0233       0.0194       0.0237       0.0776       0.0129\n",
            "     28    80        0.653        0.653       0.0192       0.0359       0.0238       0.0106       0.0094       0.0146       0.0427        0.016       0.0134        0.024       0.0953       0.0159\n",
            "     28    90        0.477        0.477       0.0161       0.0307       0.0195      0.00801       0.0107       0.0127       0.0363       0.0128       0.0142       0.0211        0.103       0.0171\n",
            "     28   100        0.368        0.368       0.0152       0.0269        0.017       0.0122        0.011       0.0134       0.0302       0.0183        0.019       0.0225       0.0816       0.0136\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     28    10        0.439        0.439       0.0163       0.0294       0.0199      0.00847      0.00991       0.0128       0.0349       0.0116       0.0136         0.02       0.0935       0.0156\n",
            "     28    20        0.538        0.538         0.02       0.0325       0.0234        0.011       0.0154       0.0166       0.0371       0.0163       0.0242       0.0258        0.075       0.0125\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              28  530.521    0.005        0.614        0.614       0.0188       0.0348       0.0226       0.0114       0.0113       0.0151       0.0408       0.0178       0.0168       0.0251       0.0878       0.0146\n",
            "! Validation         28  530.521    0.005        0.579        0.579       0.0191       0.0338       0.0216       0.0142       0.0141       0.0166       0.0382       0.0226       0.0223       0.0277       0.0852       0.0142\n",
            "Wall time: 530.52227546\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     29    10        0.709        0.709       0.0203       0.0374       0.0241       0.0139       0.0114       0.0165       0.0438       0.0199       0.0177       0.0271        0.106       0.0176\n",
            "     29    20        0.646        0.646       0.0191       0.0357       0.0218       0.0139       0.0135       0.0164       0.0406       0.0254       0.0194       0.0285       0.0778        0.013\n",
            "     29    30        0.581        0.581        0.019       0.0338       0.0218       0.0126       0.0142       0.0162       0.0391       0.0176       0.0207       0.0258       0.0849       0.0142\n",
            "     29    40        0.465        0.465       0.0169       0.0302       0.0198       0.0109       0.0114        0.014       0.0355       0.0146       0.0152       0.0218       0.0871       0.0145\n",
            "     29    50        0.656        0.656       0.0187        0.036        0.023      0.00891       0.0112       0.0144       0.0427       0.0131       0.0169       0.0242       0.0894       0.0149\n",
            "     29    60        0.696        0.696       0.0187        0.037       0.0229      0.00897       0.0114       0.0144       0.0439       0.0152        0.017       0.0253        0.106       0.0176\n",
            "     29    70        0.821        0.821       0.0212       0.0402       0.0261       0.0119       0.0105       0.0162       0.0476       0.0193       0.0169       0.0279        0.096        0.016\n",
            "     29    80        0.916        0.916       0.0214       0.0425       0.0269       0.0107       0.0101       0.0159       0.0506       0.0197       0.0145       0.0282       0.0953       0.0159\n",
            "     29    90        0.505        0.505       0.0178       0.0315       0.0215       0.0107       0.0101       0.0141        0.037       0.0167       0.0141       0.0226       0.0931       0.0155\n",
            "     29   100         0.46         0.46        0.017       0.0301       0.0192       0.0141       0.0111       0.0148       0.0344       0.0205       0.0163       0.0238        0.081       0.0135\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     29    10        0.437        0.437       0.0161       0.0293       0.0198      0.00826      0.00958       0.0125       0.0348       0.0114       0.0131       0.0198       0.0937       0.0156\n",
            "     29    20        0.545        0.545       0.0201       0.0328       0.0233       0.0114       0.0156       0.0168       0.0373       0.0168       0.0244       0.0262       0.0753       0.0126\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              29  548.621    0.005        0.614        0.614       0.0188       0.0348       0.0225       0.0114       0.0111        0.015       0.0408       0.0178       0.0167       0.0251       0.0882       0.0147\n",
            "! Validation         29  548.621    0.005        0.579        0.579       0.0191       0.0338       0.0216       0.0142       0.0139       0.0166       0.0382       0.0225       0.0223       0.0277       0.0854       0.0142\n",
            "Wall time: 548.6229950010002\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     30    10        0.803        0.803       0.0215       0.0398       0.0259       0.0119       0.0137       0.0171       0.0468       0.0192       0.0195       0.0285        0.078        0.013\n",
            "     30    20        0.856        0.856        0.021       0.0411       0.0261       0.0102       0.0117        0.016       0.0484       0.0183       0.0198       0.0288       0.0792       0.0132\n",
            "     30    30        0.529        0.529       0.0175       0.0323       0.0213       0.0101       0.0093       0.0136       0.0384       0.0137       0.0127       0.0216       0.0804       0.0134\n",
            "     30    40        0.524        0.524       0.0169       0.0321       0.0203       0.0104      0.00984       0.0135       0.0379       0.0155       0.0143       0.0226       0.0985       0.0164\n",
            "     30    50        0.574        0.574       0.0191       0.0336       0.0214        0.016       0.0132       0.0168       0.0383       0.0236       0.0188       0.0269       0.0854       0.0142\n",
            "     30    60        0.657        0.657       0.0198        0.036       0.0235       0.0117        0.013       0.0161       0.0422       0.0175       0.0185       0.0261       0.0941       0.0157\n",
            "     30    70         0.41         0.41       0.0164       0.0284       0.0182       0.0133       0.0121       0.0145       0.0321       0.0201       0.0179       0.0233       0.0705       0.0117\n",
            "     30    80        0.547        0.547       0.0181       0.0328       0.0216       0.0125      0.00986       0.0147       0.0384       0.0184       0.0154       0.0241       0.0972       0.0162\n",
            "     30    90        0.692        0.692       0.0191       0.0369       0.0223       0.0126       0.0127       0.0159       0.0429       0.0208       0.0198       0.0278       0.0835       0.0139\n",
            "     30   100        0.646        0.646       0.0187       0.0357       0.0225       0.0124      0.00988       0.0149       0.0419       0.0198       0.0149       0.0255        0.105       0.0176\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     30    10        0.438        0.438       0.0162       0.0294       0.0197      0.00832       0.0102       0.0128       0.0348       0.0116       0.0136         0.02       0.0936       0.0156\n",
            "     30    20        0.521        0.521       0.0196        0.032       0.0229       0.0109       0.0151       0.0163       0.0367       0.0154       0.0231       0.0251       0.0754       0.0126\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              30  566.715    0.005        0.613        0.613       0.0188       0.0348       0.0225       0.0115       0.0111        0.015       0.0408       0.0177       0.0167       0.0251       0.0882       0.0147\n",
            "! Validation         30  566.715    0.005        0.579        0.579       0.0191       0.0338       0.0216        0.014       0.0141       0.0166       0.0382       0.0223       0.0225       0.0277       0.0854       0.0142\n",
            "Wall time: 566.7174208030001\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     31    10        0.756        0.756       0.0196       0.0386        0.024      0.00962       0.0116       0.0151       0.0459       0.0145       0.0171       0.0258       0.0761       0.0127\n",
            "     31    20        0.539        0.539       0.0183       0.0326       0.0198        0.016       0.0143       0.0167       0.0328       0.0338       0.0305       0.0323       0.0848       0.0141\n",
            "     31    30        0.379        0.379       0.0159       0.0273       0.0176       0.0122       0.0128       0.0142        0.031       0.0163       0.0191       0.0221        0.075       0.0125\n",
            "     31    40        0.479        0.479       0.0166       0.0307       0.0191        0.012       0.0115       0.0142       0.0358       0.0183       0.0146       0.0229       0.0822       0.0137\n",
            "     31    50        0.832        0.832       0.0205       0.0405       0.0259       0.0101      0.00957       0.0152       0.0483       0.0155       0.0155       0.0265        0.109       0.0182\n",
            "     31    60        0.598        0.598       0.0179       0.0343       0.0212       0.0103       0.0121       0.0145       0.0397       0.0178       0.0207       0.0261       0.0907       0.0151\n",
            "     31    70        0.428        0.428       0.0162        0.029       0.0187       0.0118       0.0109       0.0138       0.0331       0.0203       0.0159       0.0231       0.0876       0.0146\n",
            "     31    80         0.54         0.54       0.0178       0.0326       0.0216       0.0112      0.00944       0.0141       0.0382       0.0184       0.0145       0.0237          0.1       0.0167\n",
            "     31    90        0.844        0.844       0.0207       0.0408       0.0261       0.0107      0.00878       0.0152       0.0486       0.0172        0.015       0.0269        0.102       0.0169\n",
            "     31   100        0.679        0.679       0.0198       0.0366       0.0247       0.0102      0.00957       0.0148       0.0435       0.0153       0.0146       0.0245       0.0821       0.0137\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     31    10        0.436        0.436       0.0161       0.0293       0.0197      0.00815      0.00974       0.0125       0.0348       0.0112       0.0134       0.0198       0.0934       0.0156\n",
            "     31    20        0.531        0.531       0.0197       0.0323       0.0231        0.011       0.0151       0.0164       0.0369       0.0159       0.0238       0.0255       0.0751       0.0125\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "! Train              31  584.700    0.005        0.613        0.613       0.0187       0.0347       0.0225       0.0112        0.011       0.0149       0.0408       0.0176       0.0167        0.025       0.0878       0.0146\n",
            "! Validation         31  584.700    0.005        0.577        0.577        0.019       0.0337       0.0216       0.0141       0.0138       0.0165       0.0382       0.0223       0.0221       0.0275       0.0851       0.0142\n",
            "Wall time: 584.7012025609997\n",
            "! Best model       31    0.577\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f        f_mae       f_rmse      H_f_mae      C_f_mae      N_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse     N_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
            "     32    10        0.444        0.444       0.0161       0.0296       0.0184       0.0118       0.0112       0.0138        0.034       0.0196       0.0155        0.023       0.0753       0.0126\n",
            "     32    20         0.54         0.54       0.0173       0.0326       0.0207       0.0109      0.00994       0.0138       0.0383       0.0166       0.0151       0.0233       0.0778        0.013\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nequip-train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/scripts/train.py\", line 78, in main\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/train/trainer.py\", line 778, in train\n",
            "    self.epoch_step()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/train/trainer.py\", line 916, in epoch_step\n",
            "    self.batch_step(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/train/trainer.py\", line 817, in batch_step\n",
            "    out = self.model(input_data)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/nn/_rescale.py\", line 140, in forward\n",
            "    data = self.model(data)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/nn/_grad_output.py\", line 87, in forward\n",
            "    grads = torch.autograd.grad(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 276, in grad\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nequip-train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/scripts/train.py\", line 78, in main\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/train/trainer.py\", line 778, in train\n",
            "    self.epoch_step()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/train/trainer.py\", line 916, in epoch_step\n",
            "    self.batch_step(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/train/trainer.py\", line 817, in batch_step\n",
            "    out = self.model(input_data)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/nn/_rescale.py\", line 140, in forward\n",
            "    data = self.model(data)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nequip/nn/_grad_output.py\", line 87, in forward\n",
            "    grads = torch.autograd.grad(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 276, in grad\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      LR ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         cumulative_wall ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_C_f_mae ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       training_C_f_rmse ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_H_f_mae ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       training_H_f_rmse ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_N_f_mae ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       training_N_f_rmse ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_e/N_mae ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          training_e_mae ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          training_f_mae ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         training_f_rmse ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           training_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         training_loss_f ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_psavg_f_mae ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_psavg_f_rmse ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_C_f_mae ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     validation_C_f_rmse ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_H_f_mae ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     validation_H_f_rmse ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_N_f_mae ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     validation_N_f_rmse ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_e/N_mae ‚ñÅ‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        validation_e_mae ‚ñÅ‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        validation_f_mae ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       validation_f_rmse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         validation_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       validation_loss_f ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_psavg_f_mae ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_psavg_f_rmse ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    wall ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      LR 0.005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         cumulative_wall 584.69957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_C_f_mae 0.0112\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       training_C_f_rmse 0.01758\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_H_f_mae 0.02248\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       training_H_f_rmse 0.04078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_N_f_mae 0.01097\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       training_N_f_rmse 0.01668\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        training_e/N_mae 0.01464\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          training_e_mae 0.08783\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          training_f_mae 0.01868\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         training_f_rmse 0.03473\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           training_loss 0.61254\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         training_loss_f 0.61254\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_psavg_f_mae 0.01488\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_psavg_f_rmse 0.02501\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_C_f_mae 0.0141\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     validation_C_f_rmse 0.02234\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_H_f_mae 0.02156\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     validation_H_f_rmse 0.03819\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_N_f_mae 0.01384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     validation_N_f_rmse 0.02207\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      validation_e/N_mae 0.01419\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        validation_e_mae 0.08512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        validation_f_mae 0.01903\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       validation_f_rmse 0.03372\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         validation_loss 0.57724\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       validation_loss_f 0.57724\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_psavg_f_mae 0.0165\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_psavg_f_rmse 0.02754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    wall 584.69957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mexample-run-toluene\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-538553/toluene-example/runs/qvdpzm2m?apiKey=2321139e10ca7f1703eb25e7939779e24e5b82b2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230410_142108-qvdpzm2m/logs\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nequip-deploy build --train-dir results/toluene/example-run-toluene ch2nh2-deployed.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa4pUK6D8re0",
        "outputId": "3184846b-c332-4e24-ab48-23a062d9c7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Loading best_model from training session...\n",
            "INFO:root:Compiled & optimized model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nequip-evaluate --train-dir results/toluene/example-run-toluene --batch-size 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-MI50Mo87sF",
        "outputId": "8b8fc8dd-2325-45d7-a20a-8dcb67da5989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "WARNING: please note that models running on CUDA are usually nondeterministc and that this manifests in the final test errors; for a _more_ deterministic result, please use `--device cpu`\n",
            "Loading model... \n",
            "loaded model from training session\n",
            "Loading original dataset...\n",
            "Loaded dataset specified in config.yaml.\n",
            "Using origial training dataset (3600 frames) minus training (2000 frames) and validation frames (200 frames), yielding a test set size of 1400 frames.\n",
            "Starting...\n",
            "  0% 0/1400 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "  4% 50/1400 [00:00<00:16, 80.64it/s]\n",
            "  7% 100/1400 [00:01<00:16, 78.34it/s]\n",
            " 11% 150/1400 [00:03<00:32, 38.83it/s]\n",
            "\u001b[A\n",
            " 18% 250/1400 [00:03<00:13, 82.51it/s]\n",
            "\u001b[A\n",
            " 25% 350/1400 [00:03<00:07, 133.45it/s]\n",
            "\u001b[A\n",
            " 32% 450/1400 [00:03<00:04, 191.64it/s]\n",
            "\u001b[A\n",
            " 39% 550/1400 [00:04<00:03, 253.60it/s]\n",
            "\u001b[A\n",
            " 46% 650/1400 [00:04<00:02, 315.34it/s]\n",
            "\u001b[A\n",
            " 54% 750/1400 [00:04<00:01, 375.47it/s]\n",
            "\u001b[A\n",
            " 61% 850/1400 [00:04<00:01, 429.94it/s]\n",
            "\u001b[A\n",
            " 68% 950/1400 [00:04<00:00, 471.38it/s]\n",
            "\u001b[A\n",
            " 75% 1050/1400 [00:04<00:00, 513.04it/s]\n",
            "\u001b[A\n",
            " 82% 1150/1400 [00:04<00:00, 545.72it/s]\n",
            "\u001b[A\n",
            " 89% 1250/1400 [00:05<00:00, 567.33it/s]\n",
            "\u001b[A\n",
            " 96% 1350/1400 [00:05<00:00, 580.14it/s]\n",
            "100% 1400/1400 [00:05<00:00, 260.13it/s]\n",
            "\n",
            "\n",
            "--- Final result: ---\n",
            "               f_mae =  0.006197           \n",
            "              f_rmse =  0.011105           \n",
            "             H_f_mae =  0.005164           \n",
            "             C_f_mae =  0.008359           \n",
            "             N_f_mae =  0.008164           \n",
            "         psavg_f_mae =  0.007229           \n",
            "            H_f_rmse =  0.008749           \n",
            "            C_f_rmse =  0.015080           \n",
            "            N_f_rmse =  0.014362           \n",
            "        psavg_f_rmse =  0.012730           \n",
            "               e_mae =  0.593795           \n",
            "               f_mae =  0.006197           \n",
            "              f_rmse =  0.011105           \n",
            "             H_f_mae =  0.005164           \n",
            "             C_f_mae =  0.008359           \n",
            "             N_f_mae =  0.008164           \n",
            "         psavg_f_mae =  0.007229           \n",
            "            H_f_rmse =  0.008749           \n",
            "            C_f_rmse =  0.015080           \n",
            "            N_f_rmse =  0.014362           \n",
            "        psavg_f_rmse =  0.012730           \n",
            "               e_mae =  0.593795           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nequip.ase.nequip_calculator import NequIPCalculator"
      ],
      "metadata": {
        "id": "m8ENhlbd9KOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_s0 = NequIPCalculator.from_deployed_model(f\"ch2nh2-deployed.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kqoo6ST9Rn6",
        "outputId": "73137904-94e1-4e7b-f3eb-2bf809db2562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/nequip/ase/nequip_calculator.py:73: UserWarning: Trying to use chemical symbols as NequIP type names; this may not be correct for your model! To avoid this warning, please provide `species_to_type_name` explicitly.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEzmNxig93rV",
        "outputId": "9af8f884-6016-4510-ecda-b56bb961ca00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER = \"/content/drive/MyDrive/init_models\""
      ],
      "metadata": {
        "id": "C4wTyEIv-U5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mol = read(f\"{FOLDER}/CH2NH2+_random.xyz\")"
      ],
      "metadata": {
        "id": "7niuJXOR-X9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mol.get_potential_energy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LdJREY2-a0E",
        "outputId": "be088ebf-d584-4aee-d752-a8835a6c113e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-94.46443673"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_s0.get_potential_energy(mol)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHwTxdYP-do-",
        "outputId": "a05bbad8-ce29-4ee3-f4a2-1e68e109d145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2453414648771286"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grad_pred = np.zeros((2000,18))\n",
        "enrg_pred = np.zeros(2000)\n",
        "for i in range(2000):\n",
        "  if (i % 200 ==0):\n",
        "    print(i)\n",
        "  mol.set_positions(geom[i,:])\n",
        "  ff = model_s0.get_forces(mol).astype('float64')\n",
        "  enrg_pred[i] = model_s0.get_potential_energy(mol)\n",
        "  grad_pred[i,:] = np.copy(ff).reshape(-1,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXhMdJMd-rx8",
        "outputId": "d3ca36f8-1342-49fb-eafb-7be6aa045e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "200\n",
            "400\n",
            "600\n",
            "800\n",
            "1000\n",
            "1200\n",
            "1400\n",
            "1600\n",
            "1800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "L8f34-Ql_iRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(nac_s0_s1[:2000,:],-grad_pred[:2000,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "iz_sOLBW_kTL",
        "outputId": "b1ae6a44-a371-4879-eb16-96c06b462abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f61b2afda90>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(enrg_s0[:1000],enrg_pred[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "PegzzWWLA_BV",
        "outputId": "0dd8871b-3ecf-4549-a60e-f1537ffb81d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7ff200693a00>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}